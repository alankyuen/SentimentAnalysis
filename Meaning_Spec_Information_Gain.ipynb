{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Meaning Spec Information Gain",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODIxfRFrInXe",
        "colab_type": "text"
      },
      "source": [
        "##Information theoretic motivation for using meaning specificity in negation resolution\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-67apqpdlAaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#utils\n",
        "def interpolate(raw, value_range, n_bins, eps = 1e-5):\n",
        "  '''\n",
        "  :param raw:\n",
        "    raw value to interpolate\n",
        "  :param value_range:\n",
        "    2-tuple (lower bound, upper bound)\n",
        "  :param n_bins:\n",
        "    range of bins to interpolate raw to\n",
        "  :param eps:\n",
        "    for over/underflow numerical issues\"\n",
        "  '''\n",
        "  length = value_range[1] - value_range[0]\n",
        "  adjusted_raw = raw - value_range[0] - eps\n",
        "  bin_divisor = length / float(n_bins)\n",
        "  return int(adjusted_raw/ bin_divisor)\n",
        "\n",
        "def bin_attr(lexicon_raw):\n",
        "  lexicon = []\n",
        "  for w in lexicon_raw:\n",
        "    binned_affirm = interpolate(w.affirm, affirm_range, N_AFFIRM_BINS)\n",
        "    binned_negated = interpolate(w.negated, negated_range, N_NEGATED_BINS)\n",
        "    binned_spec = interpolate(w.spec, spec_range, N_SPEC_BINS)\n",
        "    binned_word = word(w.word, binned_affirm, binned_negated, binned_spec)\n",
        "    lexicon.append(binned_word)\n",
        "  return lexicon\n",
        "\n",
        "def bin_attr2(lexicon_raw):\n",
        "  lexicon = []\n",
        "  for w in lexicon_raw:\n",
        "    binned_affirm = interpolate(w.affirm, affirm_range, N_AFFIRM_BINS)\n",
        "    binned_negated = interpolate(w.negated, negated_range, N_NEGATED_BINS)\n",
        "    binned_spec = interpolate(w.spec, spec_range, N_SPEC_BINS)\n",
        "    binned_freq = interpolate(w.freq, freq_range, N_FREQ_BINS)\n",
        "    binned_dp = interpolate(w.dp, dp_range, N_DP_BINS)\n",
        "    binned_word = word(w.word, binned_affirm, binned_negated, binned_dp, binned_freq,binned_spec)\n",
        "    lexicon.append(binned_word)\n",
        "  return lexicon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCGtMjy8CtVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import product\n",
        "from collections import namedtuple, defaultdict\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "   affirm: valence in the affirmative context\n",
        "   negated: valence in the negated context\n",
        "   freq: frequency bins F = {1,2,...,|F|}\n",
        "   dp: dispersion bins D = {1,2,...,|D|}\n",
        "   spec: meaning specificity M = {1,2,...,|M|}\n",
        "'''\n",
        "\n",
        "N_AFFIRM_BINS = 3 #\"negative\", \"neutral\", \"positive\"\n",
        "N_NEGATED_BINS = 3 #\"negative\", \"neutral\", \"positive\"\n",
        "N_FREQ_BINS = 3 #\"rare\", \"somewhat frequent\", \"frequent\"\n",
        "N_DP_BINS = 3 #\"few context\", \"some context\", \"every context\"\n",
        "N_SPEC_BINS = 3 #\"not specific\", \"some what specific\", \"very specific\"\n",
        "\n",
        "\n",
        "affirm_range = (-1.0,1.0)\n",
        "negated_range = (-1.0,1.0)\n",
        "freq_range = (0,1.0)\n",
        "dp_range = (0,1.0)\n",
        "spec_range = (0,10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKgg-EjVCqle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate information gain\n",
        "def information_gain_nga(lexicon, print_enabled = True):\n",
        "  '''\n",
        "  (SANITY CHECK)\n",
        "  I[N:A] = H[N] - H[N|A] , N: negated valence, A: affirmative score\n",
        "  H[N] = -sum(p(n)*logp(n))\n",
        "  H[N|A] = -sum_a(p(a) * sum_n(p(n|a) * logp(n|a)) )\n",
        "\n",
        "  A = AFFIRM_RANGE = {1,2,...,|A|}\n",
        "  p(n) = count(words with n) / count(words with some n in N)\n",
        "  p(a) = count(words with a) / count(words with some a in A)\n",
        "  p(n|a) = count(words with n and a) / count(words with some n in N and a)\n",
        "  '''\n",
        "  \n",
        "  #calculating H[N]\n",
        "  #p(n)\n",
        "  prob_n = {n:1e-20 for n in range(N_NEGATED_BINS)}\n",
        "  for w in lexicon:\n",
        "    prob_n[w.negated] += 1\n",
        "  for n in prob_n:\n",
        "    prob_n[n] /= float(len(lexicon)) #all words have some n\n",
        "  \n",
        "  entropy_n = -sum([prob_n[n]*math.log2(prob_n[n]) for n in range(N_NEGATED_BINS)])\n",
        "  \n",
        "  \n",
        "  #calclating H[N|A]\n",
        "  #p(a)\n",
        "  prob_a = {a:1e-20 for a in range(N_AFFIRM_BINS)}\n",
        "  for w in lexicon:\n",
        "    prob_a[w.affirm] += 1\n",
        "  for a in prob_a:\n",
        "    prob_a[a] /= float(len(lexicon)) #all words have some a\n",
        "  \n",
        "  #p(n|a)\n",
        "  n_a = set(product(range(N_NEGATED_BINS),range(N_AFFIRM_BINS)))\n",
        "  prob_nga = {(n,a):1e-20 for n,a in n_a}\n",
        "  count_a = {a:0 for a in range(N_AFFIRM_BINS)}\n",
        "  for w in lexicon:\n",
        "    prob_nga[(w.negated, w.affirm)] += 1\n",
        "    count_a[w.affirm] += 1\n",
        "\n",
        "  for n,a in prob_nga:\n",
        "    prob_nga[(n,a)] /= (float(count_a[a])+1e-20) if count_a[a] != 0 else 1\n",
        "  \n",
        "  entropy_nga = -sum([prob_a[a] * sum([prob_nga[(n,a)] * math.log2(prob_nga[(n,a)]) for n in range(N_NEGATED_BINS)]) for a in range(N_AFFIRM_BINS)])\n",
        "\n",
        "  if print_enabled: print(f\"Information gain I[A:N] = {round(entropy_n - entropy_nga,5)} = H[N]({round(entropy_n,5)}) - H[N|A]({round(entropy_nga,5)})\")\n",
        "  return entropy_n - entropy_nga\n",
        "\n",
        "def information_gain_ngam(lexicon, print_enabled = True):\n",
        "  '''\n",
        "  calculating I[M:N|A] = H[N|A] - H[N|A,M]\n",
        "  H[N|A] as calculated above\n",
        "  H[N|A,M] = -sum_m( p(m)* sum_a( p(a|m)* sum_n( p(n|a,m) * logp(n|a,m) ) ) )\n",
        "\n",
        "  p(m) = count(words with m) / count( words with some m in M)\n",
        "  p(a|m) = count(words with m and a) / count (words with some m in M and a)\n",
        "  p(n|a,m) = count(words with m and a and n) / count (words with some n in N, m and a)\n",
        "  '''\n",
        "  \n",
        "  \n",
        "  #calculating H[N|A]\n",
        "  #p(a)\n",
        "  prob_a = {a:1e-20 for a in range(N_AFFIRM_BINS)}\n",
        "  for w in lexicon:\n",
        "    prob_a[w.affirm] += 1\n",
        "  for a in prob_a:\n",
        "    prob_a[a] /= float(len(lexicon)) #all words have some a\n",
        "\n",
        "  #p(n|a)\n",
        "  n_a_pairs = set(product(range(N_NEGATED_BINS),range(N_AFFIRM_BINS)))\n",
        "  prob_nga = {(n,a):1e-20 for n,a in n_a_pairs}\n",
        "  count_a = {a:0 for a in range(N_AFFIRM_BINS)}\n",
        "  for w in lexicon:\n",
        "    prob_nga[(w.negated, w.affirm)] += 1\n",
        "    count_a[w.affirm] += 1\n",
        "\n",
        "  for n,a in prob_nga:\n",
        "    prob_nga[(n,a)] /= (float(count_a[a])+1e-20) if count_a[a] != 0 else 1\n",
        "    \n",
        "  entropy_nga = -sum([prob_a[a] * sum([prob_nga[(n,a)] * math.log2(prob_nga[(n,a)]) for n in range(N_NEGATED_BINS)]) for a in range(N_AFFIRM_BINS)])\n",
        "  \n",
        "  \n",
        "  #calculating H[N|A,M]\n",
        "  #p(m)\n",
        "  prob_m = {m:1e-20 for m in range(N_SPEC_BINS)}\n",
        "  for w in lexicon:\n",
        "    prob_m[w.spec] += 1\n",
        "  for m in prob_m:\n",
        "    prob_m[m] /= float(len(lexicon))\n",
        "\n",
        "  #p(a|m)\n",
        "  a_m = set(product(range(N_AFFIRM_BINS),range(N_SPEC_BINS)))\n",
        "  prob_agm = {(a,m):1e-10 for a,m in a_m}\n",
        "  count_m = {m:0 for m in range(N_SPEC_BINS)}\n",
        "  for w in lexicon:\n",
        "    prob_agm[(w.affirm, w.spec)] += 1\n",
        "    count_m[w.spec] += 1\n",
        "\n",
        "  for a,m in a_m:\n",
        "    prob_agm[(a,m)] /= (float(count_m[m])+1e-10) if count_m[m] != 0 else 1\n",
        "\n",
        "  #p(n|a,m)\n",
        "  n_a_m = set([(n,a,m) for n in range(N_NEGATED_BINS) for a in range(N_AFFIRM_BINS) for m in range(N_SPEC_BINS)])\n",
        "  prob_ngam = {(n,a,m):1e-10 for n,a,m in n_a_m}\n",
        "  count_a_m = {(a,m):0 for a in range(N_AFFIRM_BINS) for m in range(N_SPEC_BINS)}\n",
        "  for w in lexicon:\n",
        "    prob_ngam[(w.negated, w.affirm, w.spec)] += 1\n",
        "    count_a_m[(w.affirm, w.spec)] += 1\n",
        "\n",
        "  for n,a,m in n_a_m:\n",
        "    #if (a,m) doesn't exist, make p(n|a,m) essentially zero\n",
        "    prob_ngam[(n,a,m)] /= (float(count_a_m[(a,m)])+1e-10) if count_a_m[(a,m)] != 0 else 1\n",
        "\n",
        "  entropy_ngam = -sum([ prob_m[m]* sum([ prob_agm[(a,m)]* sum([ prob_ngam[(n,a,m)] * math.log2(prob_ngam[(n,a,m)]) for n in range(N_NEGATED_BINS)]) for a in range(N_AFFIRM_BINS)]) for m in range(N_SPEC_BINS)])\n",
        "  \n",
        "  #information gain\n",
        "  if print_enabled: print(f\"Information gain I[M:N|A] = {round(entropy_nga - entropy_ngam,5)} = H[N|A]({round(entropy_nga,5)}) - H[N|A,M]({round(entropy_ngam,5)})\")\n",
        "  \n",
        "  return entropy_nga - entropy_ngam\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et0GJWEie2Kc",
        "colab_type": "code",
        "outputId": "a9ae9cf9-9a9b-4579-ce03-9dbf9534e686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "word_raw = namedtuple(\"word_raw\", \"affirm negated freq dp\")\n",
        "word = namedtuple(\"word\", \"word affirm negated spec\")\n",
        "lexicon_raw = [     \n",
        "    word(\"promise\",       .7,       -0.9,        2),\n",
        "    word(\"sneaky\",       -.6,       -0.3,        7),\n",
        "    word(\"hallowed\",     -.2,        0.0,        9),\n",
        "    word(\"hug\",           .8,       -0.7,        2),\n",
        "    word(\"spill\",        -.7,        0.4,        4),\n",
        "    word(\"abundant\",      .7,       -0.3,        1),\n",
        "    word(\"defective\",    -.7,        0.4,        2),\n",
        "    word(\"simple\",        .6,       -0.5,        1),\n",
        "    word(\"enthusiastic\",  .8,       -0.6,        4),\n",
        "    word(\"premium\",       .8,       -0.3,        7)\n",
        "]\n",
        "\n",
        "lexicon = bin_attr(lexicon_raw)\n",
        "ignga = information_gain_nga(lexicon)\n",
        "igngam = information_gain_ngam(lexicon)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Information gain I[A:N] = 0.69546 = H[N](1.52193) - H[N|A](0.82647)\n",
            "Information gain I[M:N|A] = 0.50195 = H[N|A](0.82647) - H[N|A,M](0.32451)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kroBJZ8REelJ",
        "colab_type": "code",
        "outputId": "fc8d7cb5-05fe-4273-c35d-246dc9db2253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import random as r\n",
        "\n",
        "baselineA_ig_nga = 0.0\n",
        "baselineA_ig_ngam = 0.0\n",
        "baselineB_ig_ngam = 0.0\n",
        "\n",
        "n_trials = 1000\n",
        "for _ in range(n_trials):\n",
        "  #baseline 1\n",
        "  lexicon_rand_a = [word(w.word, r.uniform(-1,1), w.negated, r.uniform(0,1)*10) for w in lexicon_raw]\n",
        "\n",
        "  #baseline 2\n",
        "  lexicon_rand_spec = [word(w.word, w.affirm, w.negated, r.uniform(0,1)*10) for w in lexicon_raw]\n",
        "\n",
        "  lexicon = bin_attr(lexicon_rand_a)\n",
        "  baselineA_ig_nga += information_gain_nga(lexicon, print_enabled = False)\n",
        "  baselineA_ig_ngam += information_gain_ngam(lexicon, print_enabled = False)\n",
        "\n",
        "  lexicon = bin_attr(lexicon_rand_spec)\n",
        "  baselineB_ig_ngam += information_gain_ngam(lexicon, print_enabled = False)\n",
        "  \n",
        "print(f\"Baseline A(random spec and affirm)\\n\\tI[A:N] : {baselineA_ig_nga/float(n_trials)}\\n\\tI[M:N|A] : {baselineA_ig_ngam/float(n_trials)}\\n\")\n",
        "print(f\"Baseline B(random spec):\\n\\tI[M:N|A] : {baselineB_ig_ngam/float(n_trials)}\") \n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline A(random spec and affirm)\n",
            "\tI[A:N] : 0.3922487607371325\n",
            "\tI[M:N|A] : 0.5621690303177413\n",
            "\n",
            "Baseline B(random spec):\n",
            "\tI[M:N|A] : 0.3455262498180381\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}